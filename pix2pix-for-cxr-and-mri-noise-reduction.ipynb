{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport glob\nimport os\n\nfrom keras import Input\nfrom keras.applications import VGG19\nfrom keras.callbacks import TensorBoard\nfrom keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense\nfrom keras.layers import Conv2D, UpSampling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\n\nimport random\nfrom numpy import asarray\nfrom itertools import repeat\n\nimport imageio\nfrom imageio import imread\nfrom PIL import Image\nfrom skimage.transform import resize as imresize\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Tensorflow version \" + tf.__version__)\nprint(\"Keras version \" + tf.keras.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom tensorflow.keras.initializers import RandomNormal\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Conv2DTranspose, \n    LeakyReLU, Activation, \n    Concatenate, Dropout, BatchNormalization\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\nfrom tensorflow_addons.layers.normalizations import InstanceNormalization\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom scipy.linalg import sqrtm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data path\nTRAIN_PATH = r'/kaggle/input/mri-flair-png-srgan/MRI_Flair_PNG/train'\nVAL_PATH = r'/kaggle/input/mri-flair-png-srgan/MRI_Flair_PNG/val'\nTEST_PATH = r'/kaggle/input/mri-flair-png-srgan/MRI_Flair_PNG/test'\ndata_path = TRAIN_PATH\n\nepochs = 5\n\n# batch size equals to 8 (due to RAM limits)\nbatch_size = 8\n\n# define the shape of low resolution image (LR) \nlow_resolution_shape = (32, 32, 3)\n\n# define the shape of high resolution image (HR) \nhigh_resolution_shape = (256, 256, 3)\n\n# optimizer for discriminator, generator \ncommon_optimizer = Adam(0.0002, 0.5)\n\n# use seed for reproducible results\nSEED = 2020 \ntf.random.set_seed(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define hardcoded values used throughout this notebook\nIMAGE_SIZE = (256, 256)\nIMAGE_SHAPE = (256, 256, 3)\n\n\nDATASET_PATH = \"/kaggle/input/mri-flair-png-srgan/MRI_Flair_PNG\"\nTRAIN_DIR = DATASET_PATH + \"/train/\"\nVAL_DIR = DATASET_PATH + \"/val/\"\nTEST_DIR = DATASET_PATH + \"/test/\"\n\nNBR_EPOCHS = 100\nBATCH_SIZE = 5\nMAX_TRAIN_SAMPLES = 200  # restricts training data to this many samples - use for testing and for limited resources\n\n\n# get the file names in each directory, and count how many files there are\ndef only_images(ls):\n    return [f for f in ls if f.endswith('.png')]\n\ntrain_files = os.listdir(TRAIN_DIR)\nval_files = os.listdir(VAL_DIR)\n\ntrain_files = only_images(train_files)\nval_files = only_images(val_files)\n\ntrain_files = [TRAIN_DIR + \"/\" + file_path for file_path in train_files]\nval_files = [VAL_DIR + \"/\" + file_path for file_path in val_files]\n\nprint(f\"{len(train_files)} training files\")\nprint(f\"{len(val_files)} validation files\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_images(data_path):\n    image_list.extend(glob.glob(data_path + '/*'))\n    return image_list    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_images(dataset_folder_path):\n    image_folder_path = os.path.join(dataset_folder_path, \"train\")\n    image_files = os.listdir(image_folder_path)\n    image_files = [os.path.join(image_folder_path, i) for i in image_files]\n    return image_files","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_img_dims(image_list):\n    \n    min_size = []\n    max_size = []\n    \n    for i in range(len(image_list)):\n        im = Image.open(image_list[i])\n        min_size.append(min(im.size))\n        max_size.append(max(im.size))\n    \n    return min(min_size), max(max_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get min/max image sizes\n\nimage_list = train_files\nmin_size, max_size = find_img_dims(image_list)\nprint('The min and max image dims are {} and {} respectively.'\n      .format(min_size, max_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add Noise and Plot Sample","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\n\ndef add_noise(image, mean, std):\n    rows, cols, channels = image.shape\n    noise = np.random.normal(mean, std, (rows, cols, channels))\n    return np.clip(image + noise, 0, 255).astype(np.uint8)\n\ndef sample_images(image_list, batch_size, noise_mean, noise_std):\n    \"\"\"\n    Pre-process a batch of training images\n    \"\"\"\n    # image_list is the list of all images\n    # random sample a batch of images\n    images_batch = np.random.choice(image_list, size=batch_size)\n    \n    lr_images = []\n    hr_images = []\n    \n    for img in images_batch:\n        img1 = imread(img, as_gray=False, pilmode='RGB')\n        img1 = cv2.resize(img1, (256,256), cv2.INTER_AREA)\n        img1 = img1.astype(np.float32)\n        \n        # Add noise to the image\n        lr_image = add_noise(img1, noise_mean, noise_std)\n        \n        # Apply Gaussian blur filter\n        lr_image = cv2.GaussianBlur(lr_image, (5,5), 0)\n        \n        # Convert the blurred image to grayscale\n        #lr_image = cv2.cvtColor(lr_image, cv2.COLOR_BGR2GRAY)\n        \n        # Append the original and noisy image to the respective lists\n        hr_images.append(img1)\n        lr_images.append(lr_image)\n                        \n    # convert lists into numpy ndarrays\n    return np.array(hr_images), np.array(lr_images)\n\n\n# Define the parameters\nbatch_size = 3\nnoise_mean = 10\nnoise_std = 50\n\n# Sample images and preprocess them\nhr_images, lr_images = sample_images(image_list, batch_size=batch_size, noise_mean=noise_mean, noise_std=noise_std)\nhr_images = hr_images / 127.5 - 1.\nlr_images = lr_images / 127.5 - 1.\n\n# Plot the images\nfig, axs = plt.subplots(nrows=batch_size, ncols=2, figsize=(10, 6 * batch_size))\n\nfor i in range(batch_size):\n    axs[i][0].imshow(hr_images[i])\n    axs[i][0].axis('off')\n    axs[i][0].set_title('Clean-Images')\n    axs[i][1].imshow(lr_images[i])\n    axs[i][1].axis('off')\n    axs[i][1].set_title(f'Noisy-Images with noise (mean={noise_mean}, std={noise_std})')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions","metadata":{}},{"cell_type":"code","source":"# Resizing image\n\ndef resize_image(img, size):\n    new_img = cv2.resize(img, size, cv2.INTER_AREA)\n    return new_img\n\n\nimg = cv2.imread(train_files[0])\nnew_img = resize_image(img=img, size=(256, 256))\nplt.imshow(img, cmap='gray')\nplt.show()\nplt.imshow(new_img, cmap='gray')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataLoader():\n    def __init__(self, dataset_folder_path, img_size=(256, 256)):\n        \"\"\"\n        Constructs a DataLoader object\n        \n        :params:\n            dataset_folder_path (str): The path to the folder of \n                images to be loaded.  This is the name of the Pix2Pix dataset, \n                like edges2shoes.\n            image_shape (tuple): Tuple of the image dimensions like (x, y).\n        \"\"\"\n        self.dataset_folder_path = dataset_folder_path\n        self.img_size = img_size\n        self.nbr_batches = None\n\n    def load_data(self, batch_size=1, use_for_training=True):\n        \"\"\"\n        Loads data\n        \"\"\"\n        image_list = get_train_images(self.dataset_folder_path)\n        hr_images, lr_images = sample_images(image_list,\n                                             batch_size=batch_size,\n                                             noise_mean=10,\n                                             noise_std=50)\n\n        # normalize the images\n        hr_images = hr_images / 127.5 - 1.\n        lr_images = lr_images / 127.5 - 1.\n\n        return hr_images, lr_images\n\n    def load_batch(self, batch_size=1, use_for_training=True):\n        \"\"\"\n        A batch load generator\n        \"\"\"\n        if use_for_training:\n            image_folder_path = self.dataset_folder_path + \"/train\"\n        else:\n            image_folder_path = self.dataset_folder_path + \"/val\"\n\n        # get a list of the files in the given path\n        image_files = os.listdir(image_folder_path)\n        image_files = [image_folder_path + \"/\" + i for i in image_files]\n        if MAX_TRAIN_SAMPLES is not None:\n            image_files = image_files[:MAX_TRAIN_SAMPLES]\n        \n        # determine how many batches, based on the number of files \n        #   and the batch size\n        self.nbr_batches = int(len(image_files) / batch_size)\n\n        for i in range(self.nbr_batches):\n            # load data for this batch\n            hr_images, lr_images = self.load_data(batch_size, use_for_training)\n\n            yield hr_images, lr_images\n\n\n# test the data loader on a single image\nloader = DataLoader(\n    dataset_folder_path=DATASET_PATH,\n    img_size=IMAGE_SIZE\n)\ntrain_source, train_target = loader.load_data(batch_size=1, use_for_training=True)\ntrain_source.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the data loader on a batch\nloader = DataLoader(dataset_folder_path=DATASET_PATH,\n                    img_size=IMAGE_SIZE)\n\n\nfor batch_nbr, batch in enumerate(loader.load_batch(batch_size=10, use_for_training=False)):\n    print('batch nbr', batch_nbr, batch[0].shape, batch[1].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_real_samples(batch_size, nbr_patches):\n    \"\"\"\n    Uses a DataLoader object to load a batch of real image\n    samples.  Returns the source and target images, along with the\n    label (label = all ones because they are all real images).\n    \"\"\"\n    loader = DataLoader(\n        dataset_folder_path=DATASET_PATH,\n        img_size=IMAGE_SIZE\n    )\n    real_source_images, real_target_images = loader.load_data(\n        batch_size=batch_size,\n        use_for_training=True\n    )\n    y = np.ones((len(real_source_images), nbr_patches, nbr_patches, 1))\n    return [real_source_images, real_target_images], y\n\n\n# sense check: should see images here\n# [x_real_source, x_real_target], y_real = load_real_samples(batch_size=10, nbr_patches=16)\n# plt.imshow(x_real_source[0]) # source image (edges)\n# plt.show()\n# plt.imshow(x_real_target[0]) # target image (shoe)\n# plt.show()\n\n\ndef generate_fake_samples(generator, real_source_image_samples, nbr_patches):\n    \"\"\"\n    Uses a generator model to generate a batch of fake target image\n    samples.  Returns the generated images and their label (label = all\n    zeros because they are all fake images).\n    \"\"\"\n    fake_target_images = generator.predict(real_source_image_samples)\n    y = np.zeros((len(fake_target_images), nbr_patches, nbr_patches, 1))\n    return fake_target_images, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build pix2pix and Train","metadata":{}},{"cell_type":"code","source":"class Pix2Pix:\n    def __init__(self, input_image_shape):\n        \"\"\"\n        Pix2Pix constructor.\n\n        :params:\n            input_image_shape: Tuple of (rows, cols, channels),\n                where rows and cols are measured in pixels.  Pix2Pix\n                was trained on 256x256x3 shape images, originally.\n        \"\"\"\n        self.image_shape = input_image_shape\n        self.data_loader = DataLoader(\n            dataset_folder_path=DATASET_PATH,\n            img_size=input_image_shape[:2]\n        )\n        # attributes that are defined by methods\n        self.discriminator = None\n        self.generator = None\n        self.discriminator_nbr_patches = None\n        self.gan = None\n\n    def _build_discriminator(self):\n        \"\"\"\n        The discriminator is a deep CNN that takes the source image\n        and target image, and predicts whether the target image is a\n        real or fake translation of the source image.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        source_image = Input(shape=self.image_shape)\n        target_image = Input(shape=self.image_shape)\n\n        combined_images = Concatenate()([source_image, target_image])\n\n        # C64\n        x = Conv2D(\n            filters=64, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(combined_images)\n        x = LeakyReLU(alpha=0.2)(x)\n        # C128\n        x = Conv2D(\n            filters=128, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # C256\n        x = Conv2D(\n            filters=256, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # C512\n        x = Conv2D(\n            filters=512, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # second last output layer\n        x = Conv2D(\n            filters=512, kernel_size=(4, 4),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.2)(x)\n        # patch output\n        x = Conv2D(\n            filters=1, kernel_size=(4, 4),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(x)\n        patch_out = Activation('sigmoid')(x)\n        # define model\n        model = Model([source_image, target_image], patch_out)\n        # compile model\n        opt = Adam(lr=0.0002, beta_1=0.5)\n        model.compile(\n            loss='binary_crossentropy',\n            optimizer=opt,\n            loss_weights=[0.5]\n        )\n\n        return model\n\n    @staticmethod\n    def encoder_block(layer_in, nbr_filters, batch_norm=True):\n        \"\"\"\n        Encoder block in the U-net architecture that downsamples\n        the input (layer_in) using strided convolution, optionally\n        applying batch normalization, and using leaky relu activation.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        # downsampling layer\n        x = Conv2D(\n            filters=nbr_filters, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(layer_in)\n\n        if batch_norm:\n            x = BatchNormalization()(x, training=True)\n\n        x = LeakyReLU(alpha=0.2)(x)\n\n        return x\n\n    @staticmethod\n    def decoder_block(layer_in, skip_in, nbr_filters, dropout=True):\n        \"\"\"\n        Decoder block in the U-net architecture that upsamples the\n        the input (layer_in) using strided convolution, batch\n        normalization, optional dropout, and relu activation.  The\n        skip connection from the encoder must also be added, right\n        before the activation.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        # add upsampling layer\n        x = Conv2DTranspose(\n            filters=nbr_filters, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(layer_in)\n\n        # add batch normalization\n        x = BatchNormalization()(x, training=True)\n\n        # conditionally add dropout\n        if dropout:\n            x = Dropout(0.5)(x, training=True)\n\n        # merge with skip connection\n        x = Concatenate()([x, skip_in])\n\n        # relu activation\n        x = Activation('relu')(x)\n\n        return x\n\n    def _build_generator(self):\n        \"\"\"\n        The generator is an encoder-decoder model with a U-net\n        architecture.  It takes an image from the source domain\n        as input, and generates an image from the target domain\n        as output.  The tanh activation produces an output with\n        values ranged from -1 to 1.\n        \"\"\"\n        initial_weights = RandomNormal(stddev=0.02)\n\n        input_image = Input(shape=self.image_shape)\n\n        # encoder model\n        e1 = Pix2Pix.encoder_block(\n            layer_in=input_image,\n            nbr_filters=64,\n            batch_norm=False\n        )\n        e2 = Pix2Pix.encoder_block(e1, 128)\n        e3 = Pix2Pix.encoder_block(e2, 256)\n        e4 = Pix2Pix.encoder_block(e3, 512)\n        e5 = Pix2Pix.encoder_block(e4, 512)\n        e6 = Pix2Pix.encoder_block(e5, 512)\n        e7 = Pix2Pix.encoder_block(e6, 512)\n\n        # bottleneck layer\n        b = Conv2D(\n            filters=512, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(e7)\n        b = Activation('relu')(b)\n\n        # decoder model\n        d1 = Pix2Pix.decoder_block(\n            layer_in=b,\n            skip_in=e7,\n            nbr_filters=512,\n            dropout=True\n        )\n        d2 = Pix2Pix.decoder_block(d1, e6, 512)\n        d3 = Pix2Pix.decoder_block(d2, e5, 512)\n        d4 = Pix2Pix.decoder_block(d3, e4, 512, dropout=False)\n        d5 = Pix2Pix.decoder_block(d4, e3, 256, dropout=False)\n        d6 = Pix2Pix.decoder_block(d5, e2, 128, dropout=False)\n        d7 = Pix2Pix.decoder_block(d6, e1, 64, dropout=False)\n\n        # output\n        x = Conv2DTranspose(\n            filters=3, kernel_size=(4, 4),\n            strides=(2, 2),\n            padding='same',\n            kernel_initializer=initial_weights\n        )(d7)\n        output_image = Activation('tanh')(x)\n\n        # define model\n        model = Model(input_image, output_image)\n\n        return model\n\n    def _build_gan(self):\n        \"\"\"\n        Builds the Pix2Pix GAN such that the discriminator trains on\n        the real and generated images, while the generator trains\n        by means of the discriminator.  The generator updates to minimize\n        the L1 loss (error between real and fake images), and the\n        adversarial loss (error of the discriminator).  The updates\n        weight these losses at a 100:1 ratio in favor of the L1 loss,\n        in accordance with the Pix2Pix paper.\n\n        Balancing both losses requires specifying a separate model that\n        stacks the generator on the discriminator and feeds a source\n        image to both.  Since the output of the generator is also\n        the discriminator's target image (which is concatenated with the\n        source image as input), the discriminator can predict\n        whether the generated image is a real or fake translation of the\n        source image.\n        \"\"\"\n        source_image = Input(shape=self.image_shape)\n\n        # build discriminator and generator\n        self.discriminator = self._build_discriminator()\n        self.generator = self._build_generator()\n\n        # store the discriminator output number of patches\n        self.discriminator_nbr_patches = self.discriminator.output_shape[1]\n\n        # freeze discriminator weights for this combined model\n        self.discriminator.trainable = False\n\n        # map inputs and outputs\n        generator_output = self.generator(source_image)\n        discriminator_output = self.discriminator([source_image, generator_output])\n\n        # model takes source image as input,\n        # produces generated image and real/fake prediction as output\n        model = Model(source_image, [discriminator_output, generator_output])\n\n        model.compile(\n            loss=['binary_crossentropy', 'mae'],\n            optimizer=Adam(lr=0.0002, beta_1=0.5),\n            loss_weights=[1, 100]\n        )\n\n        return model\n    \n    def save_samples(self, epoch, batch_i):\n        \"\"\"\n        Saves samples of source, target, and generated images to the \n        generated_images directory.\n        \"\"\"\n        os.makedirs('generated_images/', exist_ok=True)\n        r, c = 3, 3\n        \n        # load source and target images, and generate a fake image\n        source_imgs, target_imgs = self.data_loader.load_data(batch_size=3, use_for_training=False)\n        fake_imgs = self.generator.predict(target_imgs)\n        \n        # combine them and rescale them to range from 0 to 1\n        gen_imgs = np.concatenate([target_imgs, fake_imgs, source_imgs])\n        gen_imgs = 0.5 * gen_imgs + 0.5\n        \n        titles = ['Condition', 'Generated', 'Original']\n        fig, axs = plt.subplots(r, c)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt])\n                axs[i, j].set_title(titles[i])\n                axs[i,j].axis('off')\n                cnt += 1\n        fig.savefig(\"generated_images/%d_%d.png\" % (epoch, batch_i))\n        plt.close()\n\n    def train(self, nbr_epochs, batch_size, sample_interval=100):\n        \"\"\"\n        Trains the GAN.  In each iteration, the discriminator is first\n        updated with real samples, and then with fake/generated samples.\n        Then the generator is updated.\n        \"\"\"\n        # build the GAN\n        self.gan = self._build_gan()\n        \n        for epoch in range(nbr_epochs):\n\n            print(\"Epoch\", epoch, \"of\", nbr_epochs)\n            d_loss_real_hist, d_loss_fake_hist, g_loss_hist = [], [], []\n\n            for batch_i, (x_real_source, x_real_target) in enumerate(\n                self.data_loader.load_batch(batch_size)\n            ):\n                # create corresponding real labels\n                y_real = np.ones((\n                    len(x_real_source), \n                    self.discriminator_nbr_patches,\n                    self.discriminator_nbr_patches,\n                    1\n                ))\n                \n                # generate a batch of fake samples\n                x_fake_target, y_fake = generate_fake_samples(\n                    generator=self.generator, \n                    real_source_image_samples=x_real_source, \n                    nbr_patches=self.discriminator_nbr_patches\n                )\n                \n                # update discriminator for real samples\n                d_loss_real = self.discriminator.train_on_batch(\n                    [x_real_source, x_real_target], \n                    y_real\n                )\n                d_loss_real_hist.append(d_loss_real)\n                \n                # update discriminator for generated samples\n                d_loss_fake = self.discriminator.train_on_batch(\n                    [x_real_source, x_fake_target], \n                    y_fake\n                )\n                d_loss_fake_hist.append(d_loss_fake)\n                \n                # update the generator\n                g_loss, _, _ = self.gan.train_on_batch(\n                    x_real_source, \n                    [y_real, x_real_target]\n                )\n                g_loss_hist.append(g_loss)\n                \n                # store sample images\n                if batch_i % sample_interval == 0:\n                    self.save_samples(epoch=epoch, batch_i=batch_i)\n\n            # show performance on epoch (indent this block for batch level performance)\n            print(\n                f\"Epoch {epoch+1} batch {batch_i+1} performance:\\n\", \n                f\"Discriminator real loss: {d_loss_real} \\n\",\n                f\"Discriminator fake loss: {d_loss_fake} \\n\",\n                f\"Generator loss: {g_loss} \\n\"\n            )\n\n        # return the loss hist for the final epoch\n        return d_loss_real_hist, d_loss_fake_hist, g_loss_hist\n    \n    def save_generator(self):\n        \"\"\"\n        Saves a trained Pix2Pix generator model to a h5 file.\n        \"\"\"\n        filename = \"pix2pix_generator_model.h5\"\n        self.generator.save(filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p2p = Pix2Pix(input_image_shape=IMAGE_SHAPE)\nd_loss_real, d_loss_fake, g_loss = p2p.train(\n    nbr_epochs=500, \n    batch_size=BATCH_SIZE,\n    sample_interval=5\n)\np2p.save_generator()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the loss by training iteration\nplt.plot(d_loss_real, label=\"Discriminator Loss\")\nplt.plot(g_loss, label=\"Generator Loss\")\nplt.title(\"Loss by Iteration\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Iteration\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the model and make prediction on a given image\np2p_model = load_model('pix2pix_generator_model.h5')\n#source_img = cv2.imread(train_files[199])\n#target_img = cv2.imread(train_files[199])\n\n\ndef add_noise(image, mean, std):\n    rows, cols, channels = image.shape\n    noise = np.random.normal(mean, std, (rows, cols, channels))\n    return np.clip(image + noise, 0, 255).astype(np.uint8)\n\ndef inference_images(image_list, noise_mean, noise_std):\n    \"\"\"\n    Pre-process a batch of training images\n    \"\"\"\n    # initialize empty lists\n    hr_images = []\n    lr_images = []\n    \n    # image_list is the list of all images\n    # random sample a batch of images\n    \n    img1 = cv2.imread(train_files[image_list], cv2.IMREAD_COLOR)\n    img1 = cv2.resize(img1, (256,256), cv2.INTER_AREA)\n    img1 = img1.astype(np.float32)\n\n    # Add noise to the image\n    lr_image = add_noise(img1, noise_mean, noise_std)\n\n    # Apply Gaussian blur filter\n    lr_image = cv2.GaussianBlur(lr_image, (5,5), 0)\n\n    # Append the original and noisy image to the respective lists\n    hr_images.append(img1)\n    lr_images.append(lr_image)\n                        \n    # convert lists into numpy ndarrays\n    return np.array(hr_images), np.array(lr_images)\n\n\n# Define the parameters\nimage_list = 38\nnoise_mean = 10\nnoise_std = 50\n\ntarget_img, source_img = inference_images(image_list, noise_mean, noise_std)\n\n# scale pixel values\nsource_img = source_img/127.5 - 1.\ntarget_img = target_img/127.5 - 1.\n\n# reshape\n#source_img = np.expand_dims(source_img, 0)\n#target_img = np.expand_dims(target_img, 0)\n\n# make prediction, re-scale and reshape\ngen_img = p2p_model.predict(source_img)[0]\n#gen_img = (gen_img + 1) / 2.0\ngen_img = np.expand_dims(gen_img, 0)\n\n# combine and re-scale to (0, 1)\nall_imgs = np.concatenate([source_img, target_img, gen_img])\nall_imgs = 0.5 * all_imgs + 0.5\n\ntitles = ['Input', 'Target', 'Generated']\nfor idx, i in enumerate(all_imgs):\n    plt.imshow(i)\n    plt.title(titles[idx])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}